# Allen Brain Observatory Configuration for CEBRA Training
# Dataset: Allen Brain Observatory Visual Coding - Calcium Imaging

dataset:
  name: "allen_brain_observatory_calcium"
  data_dir: "data"
  file_pattern: "*.h5"

  # Data splits
  splits:
    train: "train_mask"
    valid: "valid_mask"
    test: "test_mask"

  # Neural data configuration
  neural:
    dataset_key: "calcium_traces/df_over_f"  # (timepoints, neurons)
    dtype: "float32"
    normalize: true  # Apply z-score normalization

  # Temporal information
  temporal:
    domain_start_key: "calcium_traces/domain/start"
    domain_end_key: "calcium_traces/domain/end"
    use_timestamps: true

  # Session configuration
  sessions:
    max_sessions: null  # null = use all sessions, or specify number
    session_ids: null   # null = use all, or list specific session IDs

# CEBRA Model Configuration (poyo-ssl style)
model:
  architecture: "offset10-model"
  output_dimension: 1024  # Hidden/embedding dimension
  model_architecture: "offset10-model"

  # Self-supervised learning
  learning_mode: "time_contrastive"  # poyo-ssl style
  conditional: "time_delta"

  # Training hyperparameters
  batch_size: 4
  learning_rate: 0.0003
  temperature: 1.0
  max_iterations: 5000

  # Distance metric
  distance: "cosine"

  # Device
  device: "cuda_if_available"

  # Logging
  verbose: true

# DataLoader Configuration
dataloader:
  batch_size: 4
  num_workers: 0
  shuffle: true
  drop_last: false

  # For CEBRA's ContinuousDataLoader
  num_steps: 10
  time_offset: 10

# Training Configuration
training:
  max_epochs: 25
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001

  # Checkpointing
  checkpoint:
    enabled: true
    save_dir: "checkpoints"
    save_frequency: 10  # Save every N epochs

  # Validation
  validation:
    enabled: true
    frequency: 5  # Validate every N epochs

# HalfUNet Decoder Configuration
decoder:
  # Note: latent_dim is now automatically set to match CEBRA output_dimension
  # No need to specify latent_dim separately in two-stage training
  learning_rate: 0.0001  # Reduced from 0.001 to prevent gradient explosion
  num_epochs: 25
  batch_size: 32

  # Loss weights (6 losses for comprehensive image reconstruction)
  # Reduced weights to prevent infinity loss
  loss_weights:
    l1: 1.0           # L1 Loss - robust pixel-level reconstruction
    ssim: 0.3         # SSIM Loss - structural similarity (reduced from 0.5)
    perceptual: 0.00003  # Perceptual Loss (AlexNet) - high-level semantic features (200000 * 0.00003 = 6)
    gradient: 0.05    # Gradient Difference Loss - edge preservation (120 * 0.05 = 6)
    focal: 0.05       # Focal Loss - focus on hard examples (reduced from 0.1)
    fft: 0.1          # FFT Loss - frequency domain matching (reduced from 0.2)

# Output Configuration
output:
  save_dir: "results"
  save_embeddings: true
  save_model: true

  # Visualization
  visualize:
    enabled: true
    plot_loss: true
    plot_embeddings: true

# Weights & Biases Configuration
wandb:
  enabled: true
  entity: ""  # Enter your wandb entity here
  api_key: ""  # Enter your wandb API key here
  project: "allen-cebra-halfunet"
  name: null  # Run name (null = auto-generated)
  notes: ""  # Optional notes for this run
  tags: []  # Optional tags

# Reproducibility
seed: 42
deterministic: true

# Training mode flags
pretrain: false  # If true, combine all splits and use stable/predictable cell types
finetune: false  # If true, filter specific cell types based on ssl_mode

# Small model flag for testing
small_model: false

# SSL mode for cell type selection
ssl_mode: "predictable"  # Options: predictable, inhibitory, unpredictable, mixed
